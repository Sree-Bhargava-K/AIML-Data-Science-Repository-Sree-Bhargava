{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import min, max, mean, var, std, quantile, median\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from insolver.wrappers import InsolverGLMWrapper, InsolverGBMWrapper, InsolverRFWrapper, InsolverTrivialWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMetricsCompare:\n",
    "    \"\"\"Class for model comparison.\n",
    "    It will compute statistics and metrics for the regression task and metrics for the classification task.\n",
    "    You can compare created models with the `source` parameter or if `source` is `None` it use current working directory\n",
    "    as a source. If you want to create new models set the `create_models` parameter to True. If you already have source\n",
    "    parameter and set `create_models` parameter to `True`, new models will be added to the source list.\n",
    "    Parameters:\n",
    "        X (pd.DataFrame, pd.Series): Data for making predictions.\n",
    "        y (pd.DataFrame, pd.Series): Actual target values for X.\n",
    "        task (str, None): A task for models and metrics. If `task` new models will be created\n",
    "         Supports 'reg' and 'class'.\n",
    "        create_models (bool): If True, new models will be created and added to the comparison list.\n",
    "        source (str, list, tuple, None): List or tuple of insolver wrappers or path to the\n",
    "         folder with models. If `None`, taking current working directory as source.\n",
    "        metrics (list, tuple, callable, optional): Metrics or list of metrics to compute.\n",
    "        stats (list, tuple, callable, optional): Statistics or list of statistics to compute.\n",
    "        h2o_init_params (dict, optional): Parameters passed to `h2o.init()`, when `backend` == 'h2o'.\n",
    "        predict_params (list, optional): List of dictionaries containing parameters passed to predict methods\n",
    "         for each model.\n",
    "        features (list, optional): List of lists containing features for predict method for each model.\n",
    "        names (list, optional): List of model names.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        task=None,\n",
    "        create_models=False,\n",
    "        source=None,\n",
    "        metrics=None,\n",
    "        stats=None,\n",
    "        h2o_init_params=None,\n",
    "        predict_params=None,\n",
    "        features=None,\n",
    "        names=None,\n",
    "    ):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.task = task\n",
    "        self.create_models = create_models\n",
    "        self.source = source\n",
    "        self.metrics = [] if metrics is None else metrics\n",
    "        self.stats = stats\n",
    "        self.h2o_init_params = h2o_init_params\n",
    "        self.predict_params = predict_params\n",
    "        self.features = features\n",
    "        self.names = names\n",
    "        self.stats_results, self.metrics_results = DataFrame(), DataFrame()\n",
    "    def __repr__(self):\n",
    "        stk = traceback.extract_stack()\n",
    "        if not ('IPython' in stk[-2][0] and 'info' == stk[-2][2]):\n",
    "            import IPython.display\n",
    "            if self.task == 'reg':\n",
    "                print('Model comparison statistics:')\n",
    "                IPython.display.display(self.stats_results)\n",
    "            print('\\nModels comparison metrics:')\n",
    "            IPython.display.display(self.metrics_results)\n",
    "        else:\n",
    "            if self.task == 'reg':\n",
    "                print('Model comparison statistics:')\n",
    "                print(self.stats_results)\n",
    "            print('\\nModels comparison metrics:')\n",
    "            print(self.metrics_results)\n",
    "        return ''\n",
    "    def compare(self):\n",
    "        \"\"\"Compares models using initialized parameters.\n",
    "        If `self.create_models` == True, new models will be created and added to the source list.\n",
    "        Raises:\n",
    "            Exception: `task` parameter must be initialized and be `class` or `reg`.\n",
    "        \"\"\"\n",
    "        if self.task not in ['reg', 'class']:\n",
    "            raise Exception('Task must be \"reg\" or \"class\".')\n",
    "        if self.create_models:\n",
    "            self._init_new_models()\n",
    "        self._init_default_metrics()\n",
    "        self._init_source_models()\n",
    "        self._calc_metrics()\n",
    "        self.__repr__()\n",
    "    def _init_new_models(self):\n",
    "        \"\"\"Initializes new models using the `task` parameter.\n",
    "        If `class` then Gradient Boosting model with the catboost backend and Random Forest with the sklearn backend\n",
    "        will be created.\n",
    "        If `reg` then Gradient Boosting model with the catboost backend, Random Forest with the sklearn backend and\n",
    "        Linear Model with the sklearn backend will be created.\n",
    "        This method uses train_test_split from sklearn.model_selection, fits models with train values and changes\n",
    "        `self.X`, `self.y` to test values. Thus, when calculating metrics, it will use test values.\n",
    "        \"\"\"\n",
    "        self.source = [] if self.source is None else self.source\n",
    "        models_dict = {}\n",
    "        if self.task == 'class':\n",
    "            models_dict = {\n",
    "                'new_gbm': InsolverGBMWrapper(backend='catboost', task='class', n_estimators=10),\n",
    "                'new_rf': InsolverRFWrapper(backend='sklearn', task='class'),\n",
    "            }\n",
    "        elif self.task == 'reg':\n",
    "            models_dict = {\n",
    "                'new_glm': InsolverGLMWrapper(backend='sklearn', family=0, standardize=True),\n",
    "                'new_gbm': InsolverGBMWrapper(backend='catboost', task='reg', n_estimators=10),\n",
    "                'new_rf': InsolverRFWrapper(backend='sklearn', task='reg'),\n",
    "            }\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y)\n",
    "        for model_name in models_dict:\n",
    "            _model = models_dict[model_name]\n",
    "            _model.fit(X_train, y_train)\n",
    "            _model.algo = model_name\n",
    "            self.source.insert(0, _model)\n",
    "        self.X, self.y = X_test, y_test\n",
    "    def _init_default_metrics(self):\n",
    "        \"\"\"Initializes default metrics and adds them to the metrics list.\n",
    "        If `class` then accuracy score and f1 score will be added.\n",
    "        If `reg` then mean absolute error and r2 score will be added.\n",
    "        If `self.metrics` is callable it will be changed to the list type.\n",
    "        \"\"\"\n",
    "        if callable(self.metrics):\n",
    "            self.metrics = [self.metrics]\n",
    "        if self.task == 'class':\n",
    "            self.metrics.insert(0, accuracy_score)\n",
    "            self.metrics.insert(1, f1_score)\n",
    "        elif self.task == 'reg':\n",
    "            self.metrics.insert(0, mean_absolute_error)\n",
    "            self.metrics.insert(1, r2_score)\n",
    "    def _init_source_models(self):\n",
    "        \"\"\"Initializes source models.\n",
    "        if `source` is `None` it use current working directory as a source.\n",
    "        Raises:\n",
    "            Exception: Models with the insolver name format were not found in the current working directory.\n",
    "            TypeError: Source type is not supported.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            True if self.names is None else len(self.names) == len(self.source)\n",
    "        ), 'Check length of list containing model names.'\n",
    "        wrappers = {'glm': InsolverGLMWrapper, 'gbm': InsolverGBMWrapper, 'rf': InsolverRFWrapper}\n",
    "        if (self.source is None) or isinstance(self.source, str):\n",
    "            self.source = os.getcwd() if self.source is None else self.source\n",
    "            files = glob(os.path.join(self.source, '*'))\n",
    "            files = [file for file in files if os.path.basename(file).split('_')[0] == 'insolver']\n",
    "            if files:\n",
    "                model_list = []\n",
    "                for file in files:\n",
    "                    algo, backend = os.path.basename(file).split('_')[1:3]\n",
    "                    model_list.append(\n",
    "                        wrappers[algo](backend=backend, load_path=file)\n",
    "                        if backend != 'h2o'\n",
    "                        else wrappers[algo](backend=backend, load_path=file, h2o_init_params=self.h2o_init_params)\n",
    "                    )\n",
    "                self.models = model_list\n",
    "            else:\n",
    "                raise Exception('No models with the insolver name format found.')\n",
    "        elif isinstance(self.source, (list, tuple)):\n",
    "            self.models = self.source\n",
    "        else:\n",
    "            raise TypeError(f'Source of type {type(self.source)} is not supported.')\n",
    "    def _calc_metrics(self):\n",
    "        \"\"\"Computes metrics and statistics for the models.\n",
    "        Raises:\n",
    "            TypeError: Statistics type are not supported.\n",
    "            TypeError: Metrics type are not supported.\n",
    "        Returns:\n",
    "            Returns `None`, but results available in `self.stats`, `self.metrics`.\n",
    "        \"\"\"\n",
    "        stats_df, model_metrics = DataFrame(), DataFrame()\n",
    "        algos, backend = [], []\n",
    "        trivial = InsolverTrivialWrapper(task='reg', agg=lambda x: x)\n",
    "        trivial.fit(self.X, self.y)\n",
    "        models = [trivial] + self.models\n",
    "        features = [None] + self.features if self.features is not None else None\n",
    "        for model in models:\n",
    "            algos.append(model.algo.upper()) if hasattr(model, 'algo') else algos.append('-')\n",
    "            (\n",
    "                backend.append(model.backend.capitalize())\n",
    "                if hasattr(model, 'backend')\n",
    "                else backend.append(model.__class__.__name__)\n",
    "            )\n",
    "            p = model.predict(\n",
    "                (\n",
    "                    self.X\n",
    "                    if (features is None) or (features[models.index(model)] is None)\n",
    "                    else self.X[features[models.index(model)]]\n",
    "                ),\n",
    "                **(\n",
    "                    {}\n",
    "                    if (self.predict_params is None) or (self.predict_params[models.index(model)] is None)\n",
    "                    else self.predict_params[models.index(model)]\n",
    "                ),\n",
    "            )\n",
    "            stats_val = [mean(p), var(p), std(p), min(p), quantile(p, 0.25), median(p), quantile(p, 0.75), max(p)]\n",
    "            name_stats = ['Mean', 'Variance', 'St. Dev.', 'Min', 'Q1', 'Median', 'Q3', 'Max']\n",
    "            if self.stats is not None:\n",
    "                if isinstance(self.stats, (list, tuple)):\n",
    "                    for stat in self.stats:\n",
    "                        if callable(stat):\n",
    "                            stats_val.append(stat(p))\n",
    "                            name_stats.append(stat.__name__.replace('_', ' '))\n",
    "                        else:\n",
    "                            raise TypeError(f'Statistics with type {type(stat)} are not supported.')\n",
    "                elif callable(self.stats):\n",
    "                    stats_val.append(self.stats(p))\n",
    "                    name_stats.append(self.stats.__name__.replace('_', ' '))\n",
    "                else:\n",
    "                    raise TypeError(f'Statistics with type {type(self.stats)} are not supported.')\n",
    "            stats_df = stats_df.append(DataFrame([stats_val], columns=name_stats))\n",
    "            if (self.metrics is not None) and not models.index(model) == 0:\n",
    "                if isinstance(self.metrics, (list, tuple)):\n",
    "                    m_metrics = []\n",
    "                    for metric in self.metrics:\n",
    "                        if callable(metric):\n",
    "                            m_metrics.append(metric(self.y, p))\n",
    "                        else:\n",
    "                            raise TypeError(f'Metrics with type {type(metric)} are not supported.')\n",
    "                    metrics_names = [m.__name__.replace('_', ' ') for m in self.metrics]\n",
    "                    model_metrics = model_metrics.append(DataFrame(dict(zip(metrics_names, m_metrics), index=[0])))\n",
    "                else:\n",
    "                    raise TypeError(f'Metrics with type {type(self.metrics)} are not supported.')\n",
    "                model_metrics = model_metrics.reset_index(drop=True)\n",
    "                model_metrics = (\n",
    "                    model_metrics if 'index' not in model_metrics.columns else model_metrics.drop(['index'], axis=1)\n",
    "                )\n",
    "        model_metrics.index = list(range(len(model_metrics))) if self.names is None else self.names\n",
    "        stats_df.index = ['Actual'] + model_metrics.index.tolist()\n",
    "        stats_df[['Algo', 'Backend']] = DataFrame(\n",
    "            {'Algo': ['-'] + algos[1:], 'Backend': ['-'] + backend[1:]}, index=stats_df.index\n",
    "        )\n",
    "        model_metrics[['Algo', 'Backend']] = DataFrame(\n",
    "            {'Algo': algos[1:], 'Backend': backend[1:]}, index=model_metrics.index\n",
    "        )\n",
    "        stats_df = stats_df[list(stats_df.columns[-2:]) + list(stats_df.columns[:-2])]\n",
    "        model_metrics = model_metrics[list(model_metrics.columns[-2:]) + list(model_metrics.columns[:-2])]\n",
    "        self.stats_results = self.stats_results.append(stats_df)\n",
    "        self.metrics_results = self.metrics_results.append(model_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}