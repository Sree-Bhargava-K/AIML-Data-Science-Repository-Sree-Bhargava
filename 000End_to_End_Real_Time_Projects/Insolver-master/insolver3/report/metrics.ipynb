{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from insolver.model_tools import (\n",
    "    deviance_poisson,\n",
    "    deviance_gamma,\n",
    "    deviance_score,\n",
    "    deviance_explained,\n",
    "    deviance_explained_poisson,\n",
    "    deviance_explained_gamma,\n",
    "    lift_score,\n",
    ")\n",
    "from .error_handler import error_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_descr = 'gain curve description'\n",
    "lift_descr = 'lift curve description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler(True)\n",
    "def _create_metrics_charts(X_train, X_test, y_train, y_test, predicted_train, predicted_test, exposure=None):\n",
    "    descr_html = ''\n",
    "    try:\n",
    "        # calculate lift score\n",
    "        train_lift = lift_score(predicted_train, y_train, show=False, output=True, lift_type='quantile', q=20)\n",
    "        test_lift = lift_score(predicted_test, y_test, show=False, output=True, lift_type='quantile', q=20)\n",
    "        # generate indexes to display %\n",
    "        train_lift.index = np.arange(5, 105, 5)\n",
    "        test_lift.index = np.arange(5, 105, 5)\n",
    "        footer = {\n",
    "            'train_lift': [list(train_lift.dropna().index), list(train_lift.dropna()['Predict'])],\n",
    "            'test_lift': [list(test_lift.dropna().index), list(test_lift.dropna()['Predict'])],\n",
    "            'y_name': y_train.name,\n",
    "            'gain': 'false',\n",
    "        }\n",
    "        descr_html += f'''\n",
    "        <div class=\"p-3 m-3 bg-light border rounded-3 fw-light\">\n",
    "            <h4 class=\"text-center fw-light\">Lift Chart:</h4>\n",
    "                <div id=\"lift_score\"></div>\n",
    "                <button class=\"btn btn-primary m-3\" type=\"button\" data-bs-toggle=\"collapse\"\n",
    "                data-bs-target=\"#collapse_lift\" aria-expanded=\"False\" aria-controls=\"collapseWidthExample\">\n",
    "                    Show description\n",
    "                </button>\n",
    "                <div class=\"collapse\" id=\"collapse_lift\">\n",
    "                    <div class=\"p-3 m-3 bg-light border rounded-3 fw-light\">\n",
    "                    {lift_descr}</div>\n",
    "                </div>\n",
    "        </div>\n",
    "        '''\n",
    "    except ValueError:\n",
    "        footer = {}\n",
    "    gain = False\n",
    "    # if exposure create gain curve\n",
    "    if gain:\n",
    "        footer['gain'] = 'true'\n\n",
    "        # ideal model\n",
    "        t1, t2, t3 = gini_coef(y_train, y_train, X_train[exposure])\n",
    "        footer['train_ideal_gain'] = [list(t1), list(t2), t3]\n",
    "        t1, t2, t3 = gini_coef(y_test, y_test, X_test[exposure])\n",
    "        footer['test_ideal_gain'] = [list(t1), list(t2), t3]\n",
    "        # real model\n",
    "        t1, t2, t3 = gini_coef(y_train, predicted_train, X_train[exposure])\n",
    "        footer['train_gain'] = [list(t1), list(t2), t3]\n",
    "        t1, t2, t3 = gini_coef(y_test, predicted_test, X_test[exposure])\n",
    "        footer['test_gain'] = [list(t1), list(t2), t3]\n",
    "        descr_html += f'''\n",
    "        <div class=\"p-3 m-3 bg-light border rounded-3 fw-light\">\n",
    "            <h4 class=\"text-center fw-light\">Gain Curve:</h4>\n",
    "                <div id=\"gini_score\"></div>\n",
    "                <button class=\"btn btn-primary m-3\" type=\"button\" data-bs-toggle=\"collapse\"\n",
    "                data-bs-target=\"#collapse_gain\" aria-expanded=\"False\" aria-controls=\"collapseWidthExample\">\n",
    "                    Show description\n",
    "                </button>\n",
    "                <div class=\"collapse\" id=\"collapse_gain\">\n",
    "                    <div class=\"p-3 m-3 bg-light border rounded-3 fw-light\">\n",
    "                    {gain_descr}</div>\n",
    "                </div>\n",
    "        </div>'''\n",
    "    return footer, descr_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_psi(x_train, x_test, dataset):\n",
    "    features = x_train.columns\n",
    "    nav_items = ''\n",
    "    tab_pane_items = ''\n",
    "    for feature in features:\n",
    "        # get x values from dataset using x_test and x_train indexes\n",
    "        _x_train = dataset.loc[x_train.index]\n",
    "        _x_test = dataset.loc[x_test.index]\n",
    "        # if not categorical column, create psi\n",
    "        if _x_train[feature].dtype != object:\n",
    "            psi = stability_index(feature, _x_train, _x_test)\n",
    "            nav_class = \"nav-link active\" if feature == features[0] else \"nav-link\"\n",
    "            # replace ' ' so that href could work correctly\n",
    "            feature_replaced = feature.replace(' ', '_')\n",
    "            nav_items += f'''\n",
    "            <li class=\"nav-item\">\n",
    "                <a class=\"{nav_class}\" aria-current=\"true\" href=\"#psi_{feature_replaced}\" data-bs-toggle=\"tab\">\n",
    "                {feature}</a>\n",
    "            </li>'''\n",
    "            tab_pane_class = \"tab-pane active\" if feature == features[0] else \"tab-pane\"\n",
    "            tab_pane_items += f'''\n",
    "            <div class=\"{tab_pane_class}\" id=\"psi_{feature_replaced}\">\n",
    "                {psi.to_html(classes = \"table table-striped\", justify=\"center\")}\n",
    "            </div>\n",
    "            '''\n",
    "    return f'''\n",
    "    <div class=\"card text-center\">\n",
    "        <div class=\"card-header\">\n",
    "            <ul class=\"nav nav-tabs card-header-tabs text-nowrap p-3\" data-bs-tabs=\"tabs\"\n",
    "            style=\"overflow-x: auto;\">\n",
    "                {nav_items}\n",
    "            </ul>\n",
    "        </div>\n",
    "        <form class=\"card-body tab-content\">\n",
    "            {tab_pane_items}\n",
    "        </form>\n",
    "    </div>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_metrics(y_true, y_pred, task, metrics_to_calc, x, exposure=None):\n",
    "    \"\"\"Function to calculate metrics\n",
    "    Args:\n",
    "        y_true (1d array-like, or label indicator array / sparse matrix): Ground truth (correct) target values.\n",
    "        y_pred (1d array-like, or label indicator array / sparse matrix): Estimated targets as returned by an\n",
    "            estimator.\n",
    "        metrics_to_calc: Names of metrics to calculate, can be 'all' (all metrics will be calculated), 'main' or list.\n",
    "    Returns:\n",
    "        dict: Where keys are metrics' names and values are scores.\n",
    "    \"\"\"\n",
    "    result = dict()\n",
    "    if task == \"reg\":\n",
    "        functions = metrics_regression\n",
    "        if metrics_to_calc == 'all':\n",
    "            functions_names = metrics_regression.keys()\n",
    "        elif metrics_to_calc == 'main':\n",
    "            functions_names = functions_names_dict['reg_main']\n",
    "        elif isinstance(metrics_to_calc, list):\n",
    "            functions_names = metrics_to_calc\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f'''{type(metrics_to_calc)} type of metrics_to_calc is not supported.\n",
    "                            Must be \"all\", \"main\" or list.'''\n",
    "            )\n",
    "        result['root_mean_square_error'] = np.sqrt(functions['mean_squared_error'](y_true, y_pred))\n",
    "    elif task == \"class\":\n",
    "        # type_of_target can be continuous or binary\n",
    "        type_of_true = type_of_target(y_true)\n",
    "        type_of_pred = type_of_target(y_pred)\n",
    "        functions = metrics_classification\n",
    "        if type_of_true == 'binary' and type_of_pred == 'binary':\n",
    "            if metrics_to_calc == 'all':\n",
    "                functions_names = metrics_classification.keys()\n",
    "            elif metrics_to_calc == 'main':\n",
    "                functions_names = functions_names_dict['class_main']\n",
    "            elif isinstance(metrics_to_calc, list):\n",
    "                functions_names = metrics_to_calc\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    f'''{type(metrics_to_calc)} type of metrics_to_calc is not supported.\n",
    "                                Must be \"all\", \"main\" or list.'''\n",
    "                )\n",
    "        elif type_of_true == 'binary' and type_of_pred == 'continuous':\n",
    "            functions_names = (\n",
    "                functions_names_dict['binary_cont'] if not isinstance(metrics_to_calc, list) else metrics_to_calc\n",
    "            )\n",
    "        else:\n",
    "            raise TypeError(f\"Not supported target type <{type_of_true}> or predicted type <{type_of_pred}>\")\n",
    "    for name in functions_names:\n",
    "        if name not in functions.keys():\n",
    "            raise NotImplementedError(\n",
    "                f'''{name} metric name is not supported. Supported names for {task} task:\n",
    "                                      {functions.keys()}.'''\n",
    "            )\n",
    "        try:\n",
    "            if name == 'gini_coef' and exposure:\n",
    "                result[name] = functions[name](y_true, y_pred, x[exposure])[2]\n",
    "            else:\n",
    "                result[name] = functions[name](y_true, y_pred)\n",
    "        except Exception as e:\n",
    "            print(f'\\t-{e}')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_names_dict = {\n",
    "    'binary_cont': [\n",
    "        \"average_precision_score\",\n",
    "        \"brier_score_loss\",\n",
    "        \"det_curve\",\n",
    "        \"hinge_loss\",\n",
    "        \"log_loss\",\n",
    "        \"precision_recall_curve\",\n",
    "        \"roc_auc_score\",\n",
    "        \"roc_curve\",\n",
    "    ],\n",
    "    'class_main': [\n",
    "        'roc_auc_score',\n",
    "        'f1_score',\n",
    "        'precision_score',\n",
    "        'recall_score',\n",
    "    ],\n",
    "    'reg_main': [\n",
    "        'mean_squared_error',\n",
    "        'root_mean_squared_error',\n",
    "        'gini_coef',\n",
    "        'deviance_gaussian',\n",
    "        'deviance_poisson',\n",
    "        'deviance_gamma',\n",
    "        'deviance_explained_gaussian',\n",
    "        'deviance_explained_poisson',\n",
    "        'deviance_explained_gamma',\n",
    "        'mean_tweedie_deviance',\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coef(true, pred, exp):\n",
    "    true, pred = np.asarray(true), np.asarray(pred)\n",
    "    exp = np.asarray(exp)\n",
    "    ranking = np.argsort(-pred)\n",
    "    ranked_exposure, ranked_pure_premium = exp[ranking], true[ranking]\n",
    "    cumulated_claim_amount = np.cumsum(ranked_pure_premium * ranked_exposure)\n",
    "    cumulated_claim_amount /= cumulated_claim_amount[-1]\n",
    "    cumulated_samples = np.linspace(0, 1, len(cumulated_claim_amount))\n",
    "    gini = 2 * metrics.auc(cumulated_samples, cumulated_claim_amount) - 1\n",
    "    return cumulated_samples, cumulated_claim_amount, gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_index(scoring_variable, dev, oot, kind='psi', bins=10):\n",
    "    assert kind in ['psi', 'csi'], '\"kind\" argument must be in [\"psi\", \"csi\"]'\n",
    "    if kind == 'psi':\n",
    "        oot_bins = pd.cut(oot[scoring_variable], bins=bins)\n",
    "        dev_bins = pd.cut(dev[scoring_variable], bins=oot_bins.cat.categories)\n",
    "    else:\n",
    "        dev_bins = pd.cut(dev[scoring_variable], bins=bins)\n",
    "        oot_bins = pd.cut(oot[scoring_variable], bins=dev_bins.cat.categories)\n",
    "    psi = pd.concat(\n",
    "        [\n",
    "            (oot_bins.value_counts().sort_index(ascending=False) / oot_bins.shape[0] * 100).rename('OOT'),\n",
    "            (dev_bins.value_counts().sort_index(ascending=False) / dev_bins.shape[0] * 100).rename('DEV'),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    psi['Diff'] = psi['OOT'] - psi['DEV']\n",
    "    psi['ln_OOT_DEV'] = np.log(psi['OOT'] / psi['DEV'])\n",
    "    psi['ln_OOT_DEV'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    psi['PSI'] = psi['Diff'] * psi['ln_OOT_DEV']\n",
    "    total, total.loc[['ln_OOT_DEV', 'Diff']] = pd.Series(np.sum(psi), name='Total'), '-'\n",
    "    psi = psi.append(total).dropna()\n",
    "    return psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_regression = {\n",
    "    'explained_variance_score': metrics.explained_variance_score,\n",
    "    'max_error': metrics.max_error,\n",
    "    'mean_absolute_error': metrics.mean_absolute_error,\n",
    "    'mean_squared_error': metrics.mean_squared_error,\n",
    "    'root_mean_squared_error': lambda y_true, y_pred: np.sqrt(metrics.mean_squared_error(y_true, y_pred)),\n",
    "    'median_absolute_error': metrics.median_absolute_error,\n",
    "    'mean_absolute_percentage_error': metrics.mean_absolute_percentage_error,\n",
    "    'r2_score': metrics.r2_score,\n",
    "    'deviance_gaussian': deviance_score,\n",
    "    'deviance_poisson': deviance_poisson,\n",
    "    'deviance_gamma': deviance_gamma,\n",
    "    'deviance_explained_gaussian': deviance_explained,\n",
    "    'deviance_explained_poisson': deviance_explained_poisson,\n",
    "    'deviance_explained_gamma': deviance_explained_gamma,\n",
    "    'mean_tweedie_deviance': metrics.mean_tweedie_deviance,\n",
    "    'd2_tweedie_score': metrics.d2_tweedie_score,\n",
    "    'mean_pinball_loss': metrics.mean_pinball_loss,\n",
    "    'gini_coef': gini_coef,\n",
    "    'lift_score': lift_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_classification = {\n",
    "    \"accuracy_score\": metrics.accuracy_score,\n",
    "    \"average_precision_score\": metrics.average_precision_score,\n",
    "    \"balanced_accuracy_score\": metrics.balanced_accuracy_score,\n",
    "    \"brier_score_loss\": metrics.brier_score_loss,\n",
    "    \"classification_report\": metrics.classification_report,\n",
    "    \"cohen_kappa_score\": metrics.cohen_kappa_score,\n",
    "    \"confusion_matrix\": metrics.confusion_matrix,\n",
    "    \"det_curve\": metrics.det_curve,\n",
    "    \"f1_score\": metrics.f1_score,\n",
    "    \"hamming_loss\": metrics.hamming_loss,\n",
    "    \"hinge_loss\": metrics.hinge_loss,\n",
    "    \"jaccard_score\": metrics.jaccard_score,\n",
    "    \"log_loss\": metrics.log_loss,\n",
    "    \"matthews_corrcoef\": metrics.matthews_corrcoef,\n",
    "    \"multilabel_confusion_matrix\": metrics.multilabel_confusion_matrix,\n",
    "    \"precision_recall_curve\": metrics.precision_recall_curve,\n",
    "    \"precision_recall_fscore_support\": metrics.precision_recall_fscore_support,\n",
    "    \"precision_score\": metrics.precision_score,\n",
    "    \"recall_score\": metrics.recall_score,\n",
    "    \"roc_auc_score\": metrics.roc_auc_score,\n",
    "    \"roc_curve\": metrics.roc_curve,\n",
    "    \"zero_one_loss\": metrics.zero_one_loss,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}