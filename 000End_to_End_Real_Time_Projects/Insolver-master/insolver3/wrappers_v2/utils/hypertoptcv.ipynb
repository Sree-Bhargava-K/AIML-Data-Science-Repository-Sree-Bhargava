{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Dict, Union, Tuple, Any, Callable, Optional, Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, check_scoring, mean_squared_error\n",
    "from hyperopt import STATUS_OK, Trials, tpe, rand, fmin, space_eval, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..base import InsolverBaseWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_obj_cv(\n",
    "    params: Dict,\n",
    "    x: Union[DataFrame, Series],\n",
    "    y: Union[DataFrame, Series],\n",
    "    wrapper: InsolverBaseWrapper,\n",
    "    scoring: Callable,\n",
    "    cv: Union[None, Callable, Iterable, int] = None,\n",
    "    agg: Optional[Callable] = None,\n",
    "    maximize: bool = False,\n",
    "    **kwargs: Any\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Default hyperopt objective performing cross-validation.\n",
    "    Args:\n",
    "        params (dict): Dictionary of hyperopt parameters.\n",
    "        x (pd.DataFrame, pd.Series): Training data.\n",
    "        y (pd.DataFrame, pd.Series): Training target values.\n",
    "        wrapper (insolver.wrappers_v2.base.InsolverBaseWrapper): Wrapper containing the model.\n",
    "        scoring (callable): Metrics passed to cross_val_score calculation.\n",
    "        cv (int, iterable, cross-validation generator, optional): Cross-validation strategy from\n",
    "         sklearn. Performs 5-fold cv by default.\n",
    "        agg (callable, optional): Function computing the final score out of test cv scores.\n",
    "        maximize (bool, optional): Indicator whether to maximize or minimize objective.\n",
    "         Minimizing by default.\n",
    "        **kwargs: Other parameters passed to sklearn.model_selection.cross_val_score().\n",
    "    Returns:\n",
    "        dict: {'status': STATUS_OK, 'loss': `cv_score`}\n",
    "    \"\"\"\n",
    "    agg = mean if agg is None else agg\n",
    "    cv = KFold(n_splits=5) if cv is None else cv\n",
    "    params = {\n",
    "        key: params[key] if not (isinstance(params[key], float) and params[key].is_integer()) else int(params[key])\n",
    "        for key in params.keys()\n",
    "    }\n",
    "    estimator = wrapper.init_model(params)\n",
    "    error_score = \"raise\" if \"error_score\" not in kwargs else kwargs.pop(\"error_score\")\n",
    "    score = agg(\n",
    "        cross_val_score(\n",
    "            estimator,\n",
    "            x,\n",
    "            y=y,\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            error_score=error_score,\n",
    "            **kwargs,\n",
    "        )\n",
    "    )\n",
    "    score = -score if maximize else score\n",
    "    return {\"status\": STATUS_OK, \"loss\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_cv_proc(\n",
    "    wrapper: InsolverBaseWrapper,\n",
    "    x: Union[DataFrame, Series],\n",
    "    y: Union[DataFrame, Series],\n",
    "    params: Dict,\n",
    "    fn: Optional[Callable] = None,\n",
    "    algo: Union[None, rand.suggest, tpe.suggest] = None,\n",
    "    max_evals: int = 10,\n",
    "    timeout: Optional[int] = None,\n",
    "    fmin_params: Dict[str, Any] = None,\n",
    "    fn_params: Dict[str, Any] = None,\n",
    ") -> Tuple[Dict, Trials]:\n",
    "    \"\"\"Hyperparameter optimization using hyperopt. Using cross-validation to evaluate hyperparameters by default.\n",
    "    Args:\n",
    "        wrapper (insolver.wrappers_v2.base.InsolverBaseWrapper): Wrapper containing the model.\n",
    "        x (pd.DataFrame, pd.Series): Training data.\n",
    "        y (pd.DataFrame, pd.Series): Training target values.\n",
    "        params (dict): Dictionary of hyperparameters passed to hyperopt.\n",
    "        fn (callable, optional): Objective function to optimize with hyperopt.\n",
    "        algo (callable, optional): Algorithm for hyperopt. Available choices are: hyperopt.tpe.suggest and\n",
    "         hyperopt.random.suggest. Using hyperopt.tpe.suggest by default.\n",
    "        max_evals (int, optional): Number of function evaluations before returning.\n",
    "        timeout (None, int, optional): Limits search time by parametrized number of seconds.\n",
    "         If None, then the search process has no time constraint. None by default.\n",
    "        fmin_params (dict, optional): Dictionary of supplementary arguments for hyperopt.fmin function.\n",
    "        fn_params (dict, optional):  Dictionary of supplementary arguments for custom fn objective function.\n",
    "    Returns:\n",
    "        dict: Dictionary of the best choice of hyperparameters. Also, best model is fitted.\n",
    "    \"\"\"\n",
    "    trials = Trials()\n",
    "    algo = tpe.suggest if algo is None else algo\n",
    "    if fn is None:\n",
    "        scoring = (\n",
    "            None if not (isinstance(fn_params, dict) and (\"scoring\" in fn_params.keys())) else fn_params.pop(\"scoring\")\n",
    "        )\n",
    "        scoring = make_scorer(mean_squared_error) if scoring is None else scoring\n",
    "        try:\n",
    "            check_scoring(wrapper, scoring)\n",
    "        except ValueError:\n",
    "            scoring = make_scorer(scoring)\n",
    "        fn = functools.partial(\n",
    "            hyperopt_obj_cv,\n",
    "            x=x,\n",
    "            y=y,\n",
    "            wrapper=wrapper,\n",
    "            scoring=scoring,\n",
    "            **(fn_params if fn_params is not None else {}),\n",
    "        )\n",
    "    best = fmin(\n",
    "        fn=fn,\n",
    "        space=params,\n",
    "        trials=trials,\n",
    "        algo=algo,\n",
    "        max_evals=max_evals,\n",
    "        timeout=timeout,\n",
    "        **(fmin_params if fmin_params is not None else {}),\n",
    "    )\n",
    "    best_params = space_eval(params, best)\n",
    "    best_params = {\n",
    "        key: (\n",
    "            best_params[key]\n",
    "            if not (isinstance(best_params[key], float) and best_params[key].is_integer())\n",
    "            else int(best_params[key])\n",
    "        )\n",
    "        for key in best_params.keys()\n",
    "    }\n",
    "    return best_params, trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO_SPACE_CONFIG = {\n",
    "    \"xgboost\": {\n",
    "        \"max_depth\": hp.choice(\"max_depth\", [5, 8, 10, 12, 15]),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 0, 50),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "        \"alpha\": hp.uniform(\"alpha\", 0, 1),\n",
    "        \"lambda\": hp.uniform(\"lambda\", 0, 1),\n",
    "        \"eta\": hp.uniform(\"eta\", 0.01, 1),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.01, 1000),\n",
    "    },\n",
    "    \"lightgbm\": {\n",
    "        \"max_depth\": hp.choice(\"max_depth\", [5, 8, 10, 12, 15]),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 0, 50),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "        \"alpha\": hp.uniform(\"alpha\", 0, 1),\n",
    "        \"num_leaves\": hp.quniform(\"num_leaves\", 31, 10000, 1),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    },\n",
    "    \"catboost\": {\n",
    "        \"max_depth\": hp.choice(\"max_depth\", [5, 8, 10, 12, 15]),\n",
    "        \"min_child_samples\": hp.uniform(\"min_child_samples\", 0, 50),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"colsample_bylevel\": hp.uniform(\"colsample_bylevel\", 0.5, 1),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 2, 30),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    },\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}