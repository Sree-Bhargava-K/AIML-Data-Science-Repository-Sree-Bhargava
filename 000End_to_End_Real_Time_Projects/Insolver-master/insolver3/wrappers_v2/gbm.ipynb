{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version_info >= (3, 8):\n",
    "    from typing import Literal\n",
    "else:\n",
    "    from typing_extensions import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, Callable, Union, List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray, array, abs as npabs, mean, argsort, float64, cumsum, append, diff\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor, XGBModel, DMatrix\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor, LGBMModel\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, CatBoost, Pool, EFstrType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.graph_objects import Figure, Bar, Waterfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .base import InsolverBaseWrapper\n",
    "from .utils import save_pickle\n",
    "from .utils.hypertoptcv import hyperopt_cv_proc, tpe, rand, AUTO_SPACE_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsolverGBMWrapper(InsolverBaseWrapper):\n",
    "    \"\"\"Insolver wrapper for Gradient Boosting Machines.\n",
    "    Parameters:\n",
    "        backend (str): Framework for building GBM, currently 'xgboost', 'lightgbm' and 'catboost' are supported.\n",
    "        task (str): Task that GBM should solve: Classification or Regression. Values 'reg' and 'class' are supported.\n",
    "        n_estimators (int, optional): Number of boosting rounds. Equals 100 by default.\n",
    "        objective (str, callable): Objective function for GBM to optimize.\n",
    "        **kwargs: Parameters for GBM estimators except `n_estimators` and `objective`. Will not be changed in hyperopt.\n",
    "    \"\"\"\n",
    "    algo = 'gbm'\n",
    "    _backends = ['xgboost', 'lightgbm', 'catboost']\n",
    "    _tasks = [\"class\", \"reg\"]\n",
    "    _backend_saving_methods = {\n",
    "        'xgboost': {'pickle': save_pickle},\n",
    "        'lightgbm': {'pickle': save_pickle},\n",
    "        'catboost': {'pickle': save_pickle},\n",
    "    }\n",
    "    def __init__(\n",
    "        self,\n",
    "        backend: Optional[Literal['xgboost', 'lightgbm', 'catboost']],\n",
    "        task: Literal['class', 'reg'] = 'reg',\n",
    "        objective: Union[None, str, Callable] = None,\n",
    "        n_estimators: int = 100,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        self._get_init_args(vars())\n\n",
    "        # Checks on supported backends and tasks\n",
    "        if backend not in self._backends:\n",
    "            raise ValueError(f'Invalid \"{backend}\" backend argument. Supported backends: {self._backends}.')\n",
    "        if task not in self._tasks:\n",
    "            raise ValueError(f'Invalid \"{task}\" task argument. Supported tasks: {self._tasks}.')\n",
    "        self.backend = backend\n",
    "        self.task = task\n",
    "        self.objective = objective\n",
    "        self.n_estimators = n_estimators\n",
    "        self.kwargs = kwargs\n",
    "        self.best_params: Optional[Dict[str, Any]] = None\n",
    "        self.trials = None\n",
    "        self.model = self.init_model()\n",
    "        self.__dict__.update(self.metadata)\n",
    "    def _init_gbm_xgboost(self, **params: Any) -> XGBModel:\n",
    "        model = XGBModel()  # Just to mitigate referenced before assignment warning\n",
    "        reg_obj = [\n",
    "            'reg:squarederror',\n",
    "            'reg:squaredlogerror',\n",
    "            'reg:logistic',\n",
    "            'reg:pseudohubererror',\n",
    "            'count:poisson',\n",
    "            'survival:cox',\n",
    "            'survival:aft',\n",
    "            'reg:gamma',\n",
    "            'reg:tweedie',\n",
    "        ]\n",
    "        class_obj = ['binary:logistic', 'binary:logitraw', 'binary:hinge', 'multi:softmax', 'multi:softprob']\n",
    "        rank_obj = ['rank:ndcg', 'rank:map']\n",
    "        alias = {\n",
    "            'regression': 'reg:squarederror',\n",
    "            'logit': 'reg:logistic',\n",
    "            'binary': 'binary:logistic',\n",
    "            'multiclass': 'multi:softmax',\n",
    "            'poisson': 'count:poisson',\n",
    "            'gamma': 'reg:gamma',\n",
    "        }\n\n",
    "        # Checks on supported objectives vs tasks\n",
    "        if self.objective is None:\n",
    "            if self.task == 'reg':\n",
    "                objective: Union[str, Callable] = 'regression'\n",
    "            else:\n",
    "                objective = 'binary'\n",
    "        else:\n",
    "            objective = self.objective\n",
    "        if isinstance(objective, str):\n",
    "            objective = objective if objective not in alias.keys() else alias[objective]\n",
    "            if objective in rank_obj:\n",
    "                raise ValueError(f'Ranking objective \"{objective}\" is not supported.')\n",
    "            elif objective in reg_obj:\n",
    "                if self.task != 'reg':\n",
    "                    raise ValueError(f'Objective \"{objective}\" does not match the task \"{self.task}\".')\n",
    "            elif objective in class_obj:\n",
    "                if self.task != 'class':\n",
    "                    raise ValueError(f'Objective \"{objective}\" does not match the task \"{self.task}\".')\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f'Invalid objective \"{objective}\" supported objectives '\n",
    "                    f'{[*list(alias.keys()), *reg_obj, *class_obj]}.'\n",
    "                )\n",
    "        if self.task == 'reg':\n",
    "            model = XGBRegressor(objective=objective, n_estimators=self.n_estimators, **params)\n",
    "        if self.task == 'class':\n",
    "            model = XGBClassifier(objective=objective, n_estimators=self.n_estimators, **params)\n",
    "        return model\n",
    "    def _init_gbm_lightgbm(self, **params: Any) -> LGBMModel:\n",
    "        model = LGBMModel()  # Just to mitigate referenced before assignment warning\n",
    "        reg_obj = [\n",
    "            'regression',\n",
    "            'regression_l2',\n",
    "            'l2',\n",
    "            'mean_squared_error',\n",
    "            'mse',\n",
    "            'l2_root',\n",
    "            'root_mean_squared_error',\n",
    "            'rmse',\n",
    "            'regression_l1',\n",
    "            'l1',\n",
    "            'mean_absolute_error',\n",
    "            'mae',\n",
    "            'huber',\n",
    "            'fair',\n",
    "            'poisson',\n",
    "            'quantile',\n",
    "            'mape',\n",
    "            'mean_absolute_percentage_error',\n",
    "            'gamma',\n",
    "            'tweedie',\n",
    "        ]\n",
    "        class_obj = [\n",
    "            'binary',\n",
    "            'multiclass',\n",
    "            'softmax',\n",
    "            'multiclassova',\n",
    "            'multiclass_ova',\n",
    "            'ova',\n",
    "            'ovr',\n",
    "            'cross_entropy',\n",
    "            'xentropy',\n",
    "            'cross_entropy_lambda',\n",
    "            'xentlambda',\n",
    "        ]\n",
    "        rank_obj = ['lambdarank', 'rank_xendcg', 'xendcg', 'xe_ndcg', 'xe_ndcg_mart', 'xendcg_mart', 'rank_xendcg']\n",
    "        alias = {'logit': 'binary'}\n\n",
    "        # Checks on supported objectives vs tasks\n",
    "        if self.objective is None:\n",
    "            if self.task == 'reg':\n",
    "                objective: Union[str, Callable] = 'regression'\n",
    "            else:\n",
    "                objective = 'binary'\n",
    "        else:\n",
    "            objective = self.objective\n",
    "        if isinstance(objective, str):\n",
    "            objective = objective if objective not in alias.keys() else alias[objective]\n",
    "            if objective in rank_obj:\n",
    "                raise ValueError(f'Ranking objective \"{objective}\" is not supported.')\n",
    "            elif objective in reg_obj:\n",
    "                if self.task != 'reg':\n",
    "                    raise ValueError(f'Objective \"{objective}\" does not match the task \"{self.task}\".')\n",
    "            elif objective in class_obj:\n",
    "                if self.task != 'class':\n",
    "                    raise ValueError(f'Objective \"{objective}\" does not match the task \"{self.task}\".')\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f'Invalid objective \"{objective}\" supported objectives '\n",
    "                    f'{[*list(alias.keys()), *reg_obj, *class_obj]}.'\n",
    "                )\n",
    "        if self.task == 'reg':\n",
    "            model = LGBMRegressor(objective=objective, n_estimators=self.n_estimators, **params)\n",
    "        if self.task == 'class':\n",
    "            model = LGBMClassifier(objective=objective, n_estimators=self.n_estimators, **params)\n",
    "        return model\n",
    "    def _init_gbm_catboost(self, **params: Any) -> CatBoost:\n",
    "        model = CatBoost()  # Just to mitigate referenced before assignment warning\n",
    "        reg_obj = [\n",
    "            'MAE',\n",
    "            'MAPE',\n",
    "            'Poisson',\n",
    "            'Quantile',\n",
    "            'MultiQuantile',\n",
    "            'RMSE',\n",
    "            'RMSEWithUncertainty',\n",
    "            'LogLinQuantile',\n",
    "            'Lq',\n",
    "            'Huber',\n",
    "            'Expectile',\n",
    "            'Tweedie',\n",
    "            'LogCosh',\n",
    "            'FairLoss',\n",
    "            'NumErrors',\n",
    "            'SMAPE',\n",
    "            'R2',\n",
    "            'MSLE',\n",
    "            'MedianAbsoluteError',\n",
    "            'MultiRMSE',\n",
    "            'MultiRMSEWithMissingValues',\n",
    "        ]\n",
    "        class_obj = [\n",
    "            'Logloss',\n",
    "            'CrossEntropy',\n",
    "            'Precision',\n",
    "            'Recall',\n",
    "            'F',\n",
    "            'F1',\n",
    "            'BalancedAccuracy',\n",
    "            'BalancedErrorRate',\n",
    "            'MCC',\n",
    "            'Accuracy',\n",
    "            'CtrFactor',\n",
    "            'AUC',\n",
    "            'QueryAUC',\n",
    "            'NormalizedGini',\n",
    "            'BrierScore',\n",
    "            'HingeLoss',\n",
    "            'HammingLoss',\n",
    "            'ZeroOneLoss',\n",
    "            'Kappa',\n",
    "            'WKappa',\n",
    "            'LogLikelihoodOfPrediction',\n",
    "            'MultiClass',\n",
    "            'MultiClassOneVsAll',\n",
    "            'TotalF1',\n",
    "            'MultiLogloss',\n",
    "            'MultiCrossEntropy',\n",
    "        ]\n",
    "        rank_obj = [\n",
    "            'PairLogit',\n",
    "            'PairLogitPairwise',\n",
    "            'PairAccuracy',\n",
    "            'YetiRank',\n",
    "            'YetiRankPairwise',\n",
    "            'StochasticFilter',\n",
    "            'StochasticRank',\n",
    "            'QueryCrossEntropy',\n",
    "            'QueryRMSE',\n",
    "            'QuerySoftMax',\n",
    "            'PFound',\n",
    "            'NDCG',\n",
    "            'DCG',\n",
    "            'FilteredDCG',\n",
    "            'AverageGain',\n",
    "            'PrecisionAt',\n",
    "            'RecallAt',\n",
    "            'MAP',\n",
    "            'ERR',\n",
    "            'MRR',\n",
    "        ]\n",
    "        alias = {\n",
    "            'regression': 'RMSE',\n",
    "            'logit': 'Logloss',\n",
    "            'binary': 'Logloss',\n",
    "            'multiclass': 'MultiClass',\n",
    "            'poisson': 'Poisson',\n",
    "            'gamma': 'Tweedie:variance_power=1.9999999',\n",
    "        }\n\n",
    "        # Checks on supported objectives vs tasks\n",
    "        if self.objective is None:\n",
    "            if self.task == 'reg':\n",
    "                objective: Union[str, Callable] = 'regression'\n",
    "            else:\n",
    "                objective = 'binary'\n",
    "        else:\n",
    "            objective = self.objective\n",
    "        if isinstance(objective, str):\n",
    "            objective = objective if objective not in alias.keys() else alias[objective]\n",
    "            if objective in rank_obj:\n",
    "                raise ValueError(f'Ranking objective \"{objective}\" is not supported.')\n",
    "            elif (objective in reg_obj) or ('Tweedie:variance_power=' in objective):\n",
    "                if self.task != 'reg':\n",
    "                    raise ValueError(f'Objective \"{objective}\" does not match the task \"{self.task}\".')\n",
    "            elif objective in class_obj:\n",
    "                if self.task != 'class':\n",
    "                    raise ValueError(f'Objective \"{objective}\" does not match the task \"{self.task}\".')\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f'Invalid objective \"{objective}\" supported objectives '\n",
    "                    f'{[*list(alias.keys()), *reg_obj, *class_obj]}.'\n",
    "                )\n",
    "        def_params = dict(verbose=0, allow_writing_files=False)\n",
    "        for key, val in def_params.items():\n",
    "            if key not in params.keys():\n",
    "                params.update({key: val})\n",
    "        if self.task == 'reg':\n",
    "            model = CatBoostRegressor(objective=objective, n_estimators=self.n_estimators, **params)\n",
    "        if self.task == 'class':\n",
    "            model = CatBoostClassifier(objective=objective, n_estimators=self.n_estimators, **params)\n",
    "        return model\n",
    "    def init_model(self, additional_params: Optional[Dict] = None) -> Any:\n",
    "        model = None\n",
    "        params = self.metadata['init_params']['kwargs']\n",
    "        if additional_params is not None:\n",
    "            params.update(additional_params)\n",
    "        if self.backend == 'xgboost':\n",
    "            model = self._init_gbm_xgboost(**params)\n",
    "        if self.backend == 'lightgbm':\n",
    "            model = self._init_gbm_lightgbm(**params)\n",
    "        if self.backend == 'catboost':\n",
    "            model = self._init_gbm_catboost(**params)\n",
    "        self._update_metadata()\n",
    "        return model\n",
    "    def fit(\n",
    "        self,\n",
    "        x: Union[DataFrame, Series],\n",
    "        y: Union[DataFrame, Series],\n",
    "        report: Union[None, List, Tuple, Callable] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit a Gradient Boosting Machine.\n",
    "        Args:\n",
    "            x (pd.DataFrame, pd.Series): Training data.\n",
    "            y (pd.DataFrame, pd.Series): Training target values.\n",
    "            report (list, tuple, optional): A list of metrics to report after model fitting, optional.\n",
    "            **kwargs: Other parameters passed to Scikit-learn API .fit().\n",
    "        \"\"\"\n",
    "        for arg in [x, y]:\n",
    "            if (arg is not None) and (not isinstance(arg, (DataFrame, Series))):\n",
    "                argname = [k for k, v in locals().items() if v == arg][0]\n",
    "                raise TypeError(\n",
    "                    f'Invalid type {type(arg)} for \"{argname}\". It must be either pd.DataFrame or pd.Series.'\n",
    "                )\n",
    "        if isinstance(y, DataFrame) and y.shape[1] > 1:\n",
    "            argname = [k for k, v in locals().items() if v == y][0]\n",
    "            raise ValueError(f'Argument \"{argname}\" must be a one-dimensional DataFrame.')\n",
    "        features = list(x.columns) if isinstance(x, DataFrame) else [x.name]\n",
    "        target = list(y.columns) if isinstance(y, DataFrame) else y.name\n",
    "        self.metadata.update({'feature_names': features, 'target': target})\n",
    "        self.model.fit(x, y, **kwargs)\n",
    "        self.metadata.update({'is_fitted': True})\n",
    "        if isinstance(report, (list, tuple)) or callable(report):\n",
    "            prediction = self.model.predict(x)\n",
    "            if callable(report):\n",
    "                report_data = [[report.__name__, report(y, prediction)]]\n",
    "            else:\n",
    "                report_data = [[x.__name__, x(y, prediction)] for x in report]\n",
    "            print(DataFrame(report_data, columns=['Metrics', 'Value']).set_index('Metrics'))\n",
    "    def predict(self, x: Union[DataFrame, Series], **kwargs: Any) -> Optional[ndarray]:\n",
    "        \"\"\"Predict using GBM with feature matrix x.\n",
    "        Args:\n",
    "            x (pd.DataFrame, pd.Series): Samples.\n",
    "            **kwargs: Other parameters passed to Scikit-learn API .predict().\n",
    "        Returns:\n",
    "            array: Returns predicted values.\n",
    "        \"\"\"\n",
    "        if not self.metadata['is_fitted']:\n",
    "            raise ValueError(\"This instance is not fitted yet. Call '.fit(...)' before using this estimator.\")\n",
    "        if not isinstance(x, (DataFrame, Series)):\n",
    "            raise TypeError(f'Invalid type {type(x)} for \"x\". It must be either pd.DataFrame or pd.Series.')\n",
    "        return self.model.predict(x[self.metadata['feature_names']] if isinstance(x, DataFrame) else x, **kwargs)\n",
    "    def _calc_shap_values(self, x: Union[DataFrame, Series]) -> ndarray:\n",
    "        if not self.metadata['is_fitted']:\n",
    "            raise ValueError(\"This instance is not fitted yet. Call '.fit(...)' before using this estimator.\")\n",
    "        if not isinstance(x, (DataFrame, Series)):\n",
    "            raise TypeError(f'Invalid type {type(x)} for \"x\". It must be either pd.DataFrame or pd.Series.')\n",
    "        feature_names = self.metadata['feature_names']\n",
    "        x = DataFrame(x).T[feature_names] if isinstance(x, Series) else x[feature_names]\n",
    "        shap_values: ndarray = ndarray((0,))\n",
    "        if self.backend == 'lightgbm':\n",
    "            shap_values = self.model.predict(x, pred_contrib=True)\n",
    "        if self.backend == 'xgboost':\n",
    "            shap_values = self.model._Booster.predict(DMatrix(x), pred_contribs=True)\n",
    "        if self.backend == 'catboost':\n",
    "            shap_values = self.model.get_feature_importance(Pool(x), type=EFstrType.ShapValues)\n",
    "        return shap_values\n",
    "    def shap(self, x: Union[DataFrame, Series], show: bool = True) -> Optional[Dict[str, float64]]:\n",
    "        \"\"\"Method for SHAP feature importance estimation.\n",
    "        Args:\n",
    "            x (pd.DataFrame, pd.Series): Data for SHAP feature importance estimation.\n",
    "            show (boolean, optional): Whether to plot a graph (default: show=True).\n",
    "        Returns:\n",
    "            Dict[str, float64] containing SHAP feature importances.\n",
    "        \"\"\"\n",
    "        # Currently does not support multiclass\n",
    "        shap_values = self._calc_shap_values(x)\n",
    "        imps = mean(npabs(shap_values), axis=0)[:-1]\n",
    "        order = argsort(imps, axis=-1)\n",
    "        sorted_features_names = array(self.metadata['feature_names'])[order]\n",
    "        imps = imps[order]\n",
    "        if show:\n",
    "            fig = Figure(Bar(x=imps, y=sorted_features_names, orientation='h'))\n",
    "            fig.update_layout(\n",
    "                margin=dict(l=0, r=0, t=0, b=0),\n",
    "                xaxis=dict(title_text=\"Mean(|SHAP value|) (average impact on model output magnitude)\"),\n",
    "            )\n",
    "            fig.update_yaxes(automargin=True)\n",
    "            fig.show()\n",
    "            return None\n",
    "        else:\n",
    "            return dict(zip(sorted_features_names[::-1], imps[::-1]))\n",
    "    def shap_explain(\n",
    "        self, data: Union[DataFrame, Series], show: bool = True, layout_dict: Dict[str, Any] = None\n",
    "    ) -> Optional[Dict[str, Dict[str, float64]]]:\n",
    "        \"\"\"Method for plotting a waterfall with feature contributions or returning a dict with feature contributions.\n",
    "        Args:\n",
    "            data (pd.DataFrame, pd.Series): One-dimensional data sample for shap feature contribution calculation.\n",
    "            show (boolean, optional): Whether to plot a graph or return a json.\n",
    "            layout_dict (boolean, optional): Dictionary containing the parameters of plotly figure layout.\n",
    "        Returns:\n",
    "            None or dict: Waterfall graph or corresponding dict.\n",
    "        \"\"\"\n",
    "        # Currently does not support multiclass. Also, lacks link function. Different results with SHAP.\n",
    "        if isinstance(data, DataFrame) and (data.shape[0] != 1):\n",
    "            raise ValueError('Argument \"data\" must be a one-dimensional data sample.')\n",
    "        shap_values = self._calc_shap_values(data)[0]\n",
    "        factors, base = shap_values[:-1], shap_values[-1]\n",
    "        order = argsort(-npabs(factors))[::-1]\n",
    "        feature_names = array(self.metadata['feature_names'])[order]\n",
    "        value = data.values.reshape(-1)[order]\n",
    "        cumsum_ = cumsum(append(base, factors[order]))\n",
    "        contribution = diff(cumsum_)\n",
    "        mask_ = contribution != 0\n",
    "        feature_names = feature_names[mask_]\n",
    "        value = value[mask_]\n",
    "        contribution = contribution[mask_]\n",
    "        if show:\n",
    "            fig = Figure(\n",
    "                Waterfall(\n",
    "                    orientation='h',\n",
    "                    measure=['relative'] * contribution.shape[0],\n",
    "                    base=base,\n",
    "                    y=[f'{feature_names[i]} = {value[i]}' for i in range(len(feature_names))],\n",
    "                    x=contribution,\n",
    "                )\n",
    "            )\n",
    "            fig.add_vline(\n",
    "                x=base, annotation_text='E[f(x)]', annotation_position=\"top left\", line_width=0.2, line_dash=\"dot\"\n",
    "            )\n",
    "            fig.add_vline(\n",
    "                x=cumsum_[-1],\n",
    "                annotation_text='f(x)',\n",
    "                annotation_position=\"bottom right\",\n",
    "                line_width=0.2,\n",
    "                line_dash=\"dot\",\n",
    "            )\n",
    "            fig.update_layout(**(layout_dict if layout_dict is not None else {'margin': dict(l=0, r=0, t=0, b=0)}))\n",
    "            fig.update_yaxes(automargin=True)\n",
    "            fig.show()\n",
    "            return None\n",
    "        else:\n",
    "            explain = {f: {'value': v, 'contribution': c} for f, v, c in zip(feature_names, value, contribution)}\n",
    "            explain.update({'E[f(x)]': {'value': base, 'contribution': base}})\n",
    "            return explain\n",
    "    def hyperopt_cv(\n",
    "        self,\n",
    "        x: Union[DataFrame, Series],\n",
    "        y: Union[DataFrame, Series],\n",
    "        params: Dict[str, Any],\n",
    "        fn: Callable = None,\n",
    "        algo: Union[None, rand.suggest, tpe.suggest] = None,\n",
    "        max_evals: int = 10,\n",
    "        timeout: Optional[int] = None,\n",
    "        fmin_params: Dict[str, Any] = None,\n",
    "        fn_params: Dict[str, Any] = None,\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Hyperparameter optimization using hyperopt. Using cross-validation to evaluate hyperparameters by default.\n",
    "        Args:\n",
    "            x (pd.DataFrame, pd.Series): Training data.\n",
    "            y (pd.DataFrame, pd.Series): Training target values.\n",
    "            params (dict): Dictionary of hyperparameters passed to hyperopt.\n",
    "            fn (callable, optional): Objective function to optimize with hyperopt.\n",
    "            algo (callable, optional): Algorithm for hyperopt. Available choices are: hyperopt.tpe.suggest and\n",
    "             hyperopt.random.suggest. Using hyperopt.tpe.suggest by default.\n",
    "            max_evals (int, optional): Number of function evaluations before returning.\n",
    "            timeout (None, int, optional): Limits search time by parametrized number of seconds.\n",
    "             If None, then the search process has no time constraint. None by default.\n",
    "            fmin_params (dict, optional): Dictionary of supplementary arguments for hyperopt.fmin function.\n",
    "            fn_params (dict, optional):  Dictionary of supplementary arguments for custom fn objective function.\n",
    "        Returns:\n",
    "            dict: Dictionary of the best choice of hyperparameters. Also, best model is fitted.\n",
    "        \"\"\"\n",
    "        self.best_params, self.trials = hyperopt_cv_proc(\n",
    "            self, x, y, params, fn, algo, max_evals, timeout, fmin_params, fn_params\n",
    "        )\n",
    "        self._update_metadata()\n",
    "        self.model = self.init_model(self.best_params)\n",
    "        self.fit(\n",
    "            x, y, **({} if not ((fn_params is not None) and (\"fit_params\" in fn_params)) else fn_params[\"fit_params\"])\n",
    "        )\n",
    "        return self.best_params\n",
    "    def auto_hyperopt_cv(\n",
    "        self,\n",
    "        x: Union[DataFrame, Series],\n",
    "        y: Union[DataFrame, Series],\n",
    "        metric: Callable,\n",
    "        offset: str = None,\n",
    "        max_evals: int = 15,\n",
    "        selection: Optional[str] = None,\n",
    "        selection_thresh: float = 0.05,\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Hyperparameter optimization using hyperopt. Using cross-validation to evaluate hyperparameters by default.\n",
    "        Args:\n",
    "            x (pd.DataFrame, pd.Series): Training data.\n",
    "            y (pd.DataFrame, pd.Series): Training target values.\n",
    "            metric: Callable,\n",
    "            offset (str, optional): Column name of the offset column.\n",
    "            max_evals (int, optional): Number of function evaluations before returning.\n",
    "            selection (str, optional): Feature selection method. Currently only 'shap' method is supported.\n",
    "            selection_thresh (float, optional): Threshold for feature selection for 'shap' method. Default 0.05.\n",
    "        Returns:\n",
    "            dict: Dictionary of the best choice of hyperparameters. Also, best model is fitted.\n",
    "        \"\"\"\n",
    "        if self.backend in AUTO_SPACE_CONFIG.keys():\n",
    "            params: Dict[str, Any] = AUTO_SPACE_CONFIG[self.backend]\n",
    "            self.best_params = self.hyperopt_cv(\n",
    "                x,\n",
    "                y,\n",
    "                params,\n",
    "                max_evals=max_evals,\n",
    "                fn_params={'scoring': metric, 'fit_params': {'sample_weight': offset}},\n",
    "            )\n",
    "            if selection == 'shap':\n",
    "                shaps: Union[Dict, Series, DataFrame] = self.shap(x, show=False)\n",
    "                shaps = DataFrame.from_dict({'shap': shaps}).abs().sort_values('shap', ascending=False)\n",
    "                shaps = shaps / shaps.sum()\n",
    "                columns = shaps[shaps['shap'] >= selection_thresh].index.tolist()\n",
    "                self.best_params = self.hyperopt_cv(\n",
    "                    x[columns],\n",
    "                    y,\n",
    "                    params,\n",
    "                    max_evals=max_evals,\n",
    "                    fn_params={'scoring': metric, 'fit_params': {'sample_weight': offset}},\n",
    "                )\n",
    "            return self.best_params\n",
    "        else:\n",
    "            return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}