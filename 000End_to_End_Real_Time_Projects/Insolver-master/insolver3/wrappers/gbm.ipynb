{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import cumsum, diff, exp, true_divide, add, append, nan, concatenate, array, abs as npabs\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, SCORERS\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from shap import TreeExplainer, summary_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.graph_objects import Figure, Waterfall\n",
    "from plotly.io import to_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .base import InsolverBaseWrapper\n",
    "from .extensions import InsolverCVHPExtension, InsolverPDPExtension\n",
    "from .extensions.cvnhp import AUTO_SPACE_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsolverGBMWrapper(InsolverBaseWrapper, InsolverCVHPExtension, InsolverPDPExtension):\n",
    "    \"\"\"Insolver wrapper for Gradient Boosting Machines.\n",
    "    Parameters:\n",
    "        backend (str): Framework for building GBM, 'xgboost', 'lightgbm' and 'catboost' are supported.\n",
    "        task (str): Task that GBM should solve: Classification or Regression. Values 'reg' and 'class' are supported.\n",
    "        n_estimators (int, optional): Number of boosting rounds. Equals 100 by default.\n",
    "        objective (str, callable): Objective function for GBM to optimize.\n",
    "        load_path (str, optional): Path to GBM model to load from disk.\n",
    "        **kwargs: Parameters for GBM estimators except `n_estimators` and `objective`. Will not be changed in hyperopt.\n",
    "    \"\"\"\n",
    "    def __init__(self, backend, task=None, objective=None, n_estimators=100, load_path=None, **kwargs):\n",
    "        super(InsolverGBMWrapper, self).__init__(backend)\n",
    "        self.init_args = self._get_init_args(vars())\n",
    "        self.algo, self._backends = 'gbm', ['xgboost', 'lightgbm', 'catboost']\n",
    "        self._tasks = ['class', 'reg']\n",
    "        self._back_load_dict = {\n",
    "            'xgboost': self._pickle_load,\n",
    "            'lightgbm': self._pickle_load,\n",
    "            'catboost': self._pickle_load,\n",
    "        }\n",
    "        self._back_save_dict = {\n",
    "            'xgboost': self._pickle_save,\n",
    "            'lightgbm': self._pickle_save,\n",
    "            'catboost': self._pickle_save,\n",
    "        }\n",
    "        self.n_estimators, self.objective, self.params = n_estimators, objective, None\n",
    "        if backend not in self._backends:\n",
    "            raise NotImplementedError(f'Error with the backend choice. Supported backends: {self._backends}')\n",
    "        if load_path is not None:\n",
    "            self.load_model(load_path)\n",
    "        else:\n",
    "            if task in self._tasks:\n",
    "                gbm_init = {\n",
    "                    'class': {'xgboost': XGBClassifier, 'lightgbm': LGBMClassifier, 'catboost': CatBoostClassifier},\n",
    "                    'reg': {'xgboost': XGBRegressor, 'lightgbm': LGBMRegressor, 'catboost': CatBoostRegressor},\n",
    "                }\n",
    "                objectives = {\n",
    "                    'regression': {'xgboost': 'reg:squarederror', 'lightgbm': 'regression', 'catboost': 'RMSE'},\n",
    "                    'binary': {'xgboost': 'binary:logistic', 'lightgbm': 'binary', 'catboost': 'Logloss'},\n",
    "                    'multiclass': {'xgboost': 'multi:softmax', 'lightgbm': 'multiclass', 'catboost': 'MultiClass'},\n",
    "                    'poisson': {'xgboost': 'count:poisson', 'lightgbm': 'poisson', 'catboost': 'Poisson'},\n",
    "                    'gamma': {\n",
    "                        'xgboost': 'reg:gamma',\n",
    "                        'lightgbm': 'gamma',\n",
    "                        'catboost': 'Tweedie:variance_power=1.9999999',\n",
    "                    },\n",
    "                }\n",
    "                self.objective_ = (\n",
    "                    objectives[self.objective][self.backend] if self.objective in objectives.keys() else self.objective\n",
    "                )\n",
    "                kwargs.update({'objective': self.objective_, 'n_estimators': self.n_estimators})\n",
    "                self.model, self.params = gbm_init[task][self.backend](**(kwargs if kwargs is not None else {})), kwargs\n",
    "                def __params_gbm(**params):\n",
    "                    params.update(self.params)\n",
    "                    return gbm_init[task][self.backend](**params)\n",
    "                self.object = __params_gbm\n",
    "            else:\n",
    "                raise NotImplementedError(f'Task parameter supports values in {self._tasks}.')\n",
    "        self._update_meta()\n",
    "    def fit(self, X, y, report=None, **kwargs):\n",
    "        \"\"\"Fit a Gradient Boosting Machine.\n",
    "        Args:\n",
    "            X (pd.DataFrame, pd.Series): Training data.\n",
    "            y (pd.DataFrame, pd.Series): Training target values.\n",
    "            report (list, tuple, optional): A list of metrics to report after model fitting, optional.\n",
    "            **kwargs: Other parameters passed to Scikit-learn API .fit().\n",
    "        \"\"\"\n",
    "        self.model.fit(X, y, **kwargs)\n",
    "        if not hasattr(self.model, 'feature_name_'):\n",
    "            self.model.feature_name_ = X.columns if isinstance(X, DataFrame) else [X.name]\n",
    "        self._update_meta()\n",
    "        if report is not None:\n",
    "            if isinstance(report, (list, tuple)):\n",
    "                prediction = self.model.predict(X)\n",
    "                print(\n",
    "                    DataFrame([[x.__name__, x(y, prediction)] for x in report])\n",
    "                    .rename({0: 'Metrics', 1: 'Value'}, axis=1)\n",
    "                    .set_index('Metrics')\n",
    "                )\n",
    "    def predict(self, X, **kwargs):\n",
    "        \"\"\"Predict using GBM with feature matrix X.\n",
    "        Args:\n",
    "            X (pd.DataFrame, pd.Series): Samples.\n",
    "            **kwargs: Other parameters passed to Scikit-learn API .predict().\n",
    "        Returns:\n",
    "            array: Returns predicted values.\n",
    "        \"\"\"\n",
    "        return self.model.predict(\n",
    "            X if not hasattr(self.model, 'feature_name_') else X[self.model.feature_name_], **kwargs\n",
    "        )\n",
    "    def shap(self, X, show=False, plot_type='bar'):\n",
    "        \"\"\"Method for shap values calculation and corresponding plot of feature importances.\n",
    "        Args:\n",
    "            X (pd.DataFrame, pd.Series): Data for shap values calculation.\n",
    "            show (boolean, optional): Whether to plot a graph.\n",
    "            plot_type (str, optional): Type of feature importance graph, takes value in ['dot', 'bar'].\n",
    "        Returns:\n",
    "            JSON containing shap values.\n",
    "        \"\"\"\n",
    "        explainer = TreeExplainer(self.model)\n",
    "        X = DataFrame(X).T if isinstance(X, Series) else X\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        shap_values = shap_values[0] if isinstance(shap_values, list) and (len(shap_values) == 2) else shap_values\n",
    "        variables = list(X.columns)\n",
    "        mean_shap = npabs(shap_values).mean(axis=0).tolist()\n",
    "        if show:\n",
    "            summary_plot(shap_values, X, plot_type=plot_type, feature_names=variables)\n",
    "        return {variables[i]: mean_shap[i] for i in range(len(variables))}\n",
    "    def shap_explain(self, data, index=None, link=None, show=True, layout_dict=None):\n",
    "        \"\"\"Method for plotting a waterfall graph or return corresponding JSON if show=False.\n",
    "        Args:\n",
    "            data (pd.DataFrame, pd.Series): Data for shap values calculation.\n",
    "            index (int, optional): Index of the observation of interest, if data is pd.DataFrame.\n",
    "            link (callable, optional): A function for transforming shap values into predictions.\n",
    "              Unnecessary if self.objective is present and it takes values in ['binary', 'poisson', 'gamma'].\n",
    "            show (boolean, optional): Whether to plot a graph or return a json.\n",
    "            layout_dict (boolean, optional): Dictionary containing the parameters of plotly figure layout.\n",
    "        Returns:\n",
    "            None or dict: Waterfall graph or corresponding JSON.\n",
    "        \"\"\"\n",
    "        def logit(x):\n",
    "            return true_divide(1, add(1, exp(x)))\n",
    "        explainer = TreeExplainer(self.model)\n",
    "        if isinstance(self.model, (XGBClassifier, XGBRegressor)):\n",
    "            feature_names = self.model.get_booster().feature_names\n",
    "        elif isinstance(self.model, (LGBMClassifier, LGBMRegressor)):\n",
    "            feature_names = self.model.feature_name_\n",
    "        elif isinstance(self.model, (CatBoostClassifier, CatBoostRegressor)):\n",
    "            feature_names = self.model.feature_names_\n",
    "        else:\n",
    "            raise NotImplementedError(f'Error with the backend choice. Supported backends: {self._backends}')\n",
    "        index = index if (isinstance(data, DataFrame)) and (index is not None) else None\n",
    "        data = DataFrame(data).T[feature_names] if isinstance(data, Series) else data[feature_names]\n",
    "        data = data if index is None else data.loc[[index], :]\n",
    "        shap_values = explainer.shap_values(data)\n",
    "        cond_bool = isinstance(shap_values, list) and (len(shap_values) == 2)\n",
    "        shap_values = shap_values[0] if cond_bool else shap_values\n",
    "        expected_value = explainer.expected_value[0] if cond_bool else explainer.expected_value\n",
    "        prediction = DataFrame(\n",
    "            [expected_value] + shap_values.reshape(-1).tolist(),\n",
    "            index=['Intercept'] + feature_names,\n",
    "            columns=['SHAP Value'],\n",
    "        )\n",
    "        prediction['CumSum'] = cumsum(prediction['SHAP Value'])\n",
    "        prediction['Value'] = append(nan, data.values.reshape(-1))\n",
    "        if (self.objective is not None) and (link is None):\n",
    "            link = exp if self.objective in ['poisson', 'gamma'] else logit if self.objective == 'binary' else None\n",
    "        if link is not None:\n",
    "            prediction['Link'] = link(prediction['CumSum'])\n",
    "            prediction['Contribution'] = [link(expected_value)] + list(diff(prediction['Link']))\n",
    "        else:\n",
    "            prediction['Contribution'] = [expected_value] + list(diff(prediction['CumSum']))\n",
    "        fig = Figure(\n",
    "            Waterfall(\n",
    "                name=f'Prediction {index}',\n",
    "                orientation='h',\n",
    "                measure=['relative'] * len(prediction),\n",
    "                y=[\n",
    "                    prediction.index[i] if i == 0 else f'{prediction.index[i]}={data.values.reshape(-1)[i-1]}'\n",
    "                    for i in range(len(prediction.index))\n",
    "                ],\n",
    "                x=prediction['Contribution'],\n",
    "            )\n",
    "        )\n",
    "        fig.update_layout(**(layout_dict if layout_dict is not None else {}))\n",
    "        if show:\n",
    "            fig.show()\n",
    "        else:\n",
    "            json_ = prediction[['Value', 'SHAP Value', 'Contribution']].T.to_dict()\n",
    "            fig_base64 = b64encode(to_image(fig, format='jpeg')).decode('ascii')\n",
    "            json_.update(\n",
    "                {'id': int(data.index.values), 'predict': prediction['Link'][-1], \"ShapValuesPlot\": fig_base64}\n",
    "            )\n",
    "            return json_\n",
    "    def cross_val(self, X, y, scoring=None, cv=None, **kwargs):\n",
    "        \"\"\"Method for performing cross-validation given the hyperparameters of initialized or fitted model.\n",
    "        Args:\n",
    "            X (pd.DataFrame, pd.Series): Training data.\n",
    "            y (pd.DataFrame, pd.Series): Training target values.\n",
    "            scoring (callable): Metrics passed to sklearn.model_selection.cross_validate calculation.\n",
    "            cv (int, cross-validation generator or an iterable`, optional): Cross-validation strategy from\n",
    "             sklearn. Performs 5-fold cv by default.\n",
    "            **kwargs: Other parameters passed to sklearn.model_selection.cross_validate.\n",
    "        Returns:\n",
    "            pd.DataFrame, pd.DataFrame: DataFrame with metrics on folds, DataFrame with shap values on folds.\n",
    "        \"\"\"\n",
    "        scoring = mean_squared_error if scoring is None else scoring\n",
    "        models, metrics = self._cross_val(X, y, scoring=scoring, cv=cv, **kwargs)\n",
    "        if callable(scoring):\n",
    "            scorers = {scoring.__name__.replace('_', ' '): array([scoring(y, self.model.predict(X))])}\n",
    "        elif isinstance(scoring, (tuple, list)):\n",
    "            scorers = {\n",
    "                scorer.__name__.replace('_', ' '): array([scorer(y, self.model.predict(X))]) for scorer in scoring\n",
    "            }\n",
    "        elif isinstance(scoring, str):\n",
    "            if scoring in SCORERS:\n",
    "                scorers = {scoring.replace('_', ' '): array([SCORERS[scoring](self.model, X=X, y=y)])}\n",
    "            else:\n",
    "                raise ValueError(f'Scorer {scoring} is not supported.')\n",
    "        else:\n",
    "            raise NotImplementedError(f'Scoring of type {scoring} is not supported')\n",
    "        metrics = DataFrame({key: concatenate((scorers[key], metrics[key])) for key in scorers.keys()}).T\n",
    "        metrics.columns = [f'Fold {i}' if i != 0 else 'Overall' for i in range(metrics.shape[1])]\n",
    "        shap_coefs = []\n",
    "        explainer = TreeExplainer(self.model)\n",
    "        shap_coefs.append(\n",
    "            ([explainer.expected_value] if explainer.expected_value is None else explainer.expected_value.tolist())\n",
    "            + explainer.shap_values(X).mean(axis=0).tolist()\n",
    "        )\n",
    "        for model in models:\n",
    "            explainer = TreeExplainer(model)\n",
    "            shap_coefs.append(\n",
    "                ([explainer.expected_value] if explainer.expected_value is None else explainer.expected_value.tolist())\n",
    "                + explainer.shap_values(X).mean(axis=0).tolist()\n",
    "            )\n",
    "        shapdf = DataFrame(\n",
    "            array(shap_coefs).T,\n",
    "            columns=['Overall'] + [f'Fold {x}' for x in range(1, len(models) + 1)],\n",
    "            index=['Intercept'] + X.columns.tolist(),\n",
    "        )\n",
    "        return metrics, shapdf\n",
    "    def auto(self, x_train, y_train, metric, offset=None, selection='shap', selection_thresh=0.05):\n",
    "        self.hyperopt_cv(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            AUTO_SPACE_CONFIG[self.backend],\n",
    "            max_evals=50,\n",
    "            fn_params={'scoring': metric, 'fit_params': {'sample_weight': offset}},\n",
    "        )\n",
    "        if selection:\n",
    "            shaps = self.shap(x_train)\n",
    "            shaps = DataFrame.from_dict({'shap': shaps}).abs().sort_values('shap', ascending=False)\n",
    "            shaps = shaps / shaps.sum()\n",
    "            columns = shaps[shaps['shap'] >= selection_thresh].index.tolist()\n",
    "            self.hyperopt_cv(\n",
    "                x_train[columns],\n",
    "                y_train,\n",
    "                AUTO_SPACE_CONFIG[self.backend],\n",
    "                max_evals=50,\n",
    "                fn_params={'scoring': metric, 'fit_params': {'sample_weight': offset}},\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}