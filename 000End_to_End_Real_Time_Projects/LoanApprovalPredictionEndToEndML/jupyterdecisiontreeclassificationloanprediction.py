# -*- coding: utf-8 -*-
"""JupyterDecisionTreeClassificationLoanPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aQtRThn93HleI5cEb0Vqmg-JmmzT8IyY

# If you like this work, please upvote this kernel as it will keep me motivated to do more in the future and share the kernel with others so we can all benefit from it.
"""

#mport numpy as np # linear algebra
#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

#import os
#for dirname, _, filenames in os.walk(r"C:\Users\sreeb\Downloads\AIMLAcademics\000End_to_End_Real_Time_Projects\DomainBasedDataScienceProjects\BankingDomain\LoanPredictionProblem\DataSets\"):
#    for filename in filenames:
#        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

################### Importing Libraries ######################
import pandas as pd

train_df = pd.read_csv(r"C:\Users\sreeb\Downloads\AIMLAcademics\000End_to_End_Real_Time_Projects\DomainBasedDataScienceProjects\BankingDomain\LoanPredictionProblem\DataSets\train.csv")
train_df.info()

############ Count number of Categorical and Numerical Columns ######################
train_df = train_df.drop(columns=['Loan_ID']) ## Dropping Loan ID
categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']
#categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Loan_Amount_Term']

print(categorical_columns)
numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']
print(numerical_columns)

### Data Visualization libraries
import seaborn as sns
import matplotlib.pyplot as plt


fig,axes = plt.subplots(4,2,figsize=(12,15))
for idx,cat_col in enumerate(categorical_columns):
    row,col = idx//2,idx%2
    sns.countplot(x=cat_col,data=train_df,hue='Loan_Status',ax=axes[row,col])


plt.subplots_adjust(hspace=1)

fig,axes = plt.subplots(1,3,figsize=(17,5))
for idx,cat_col in enumerate(numerical_columns):
    sns.boxplot(y=cat_col,data=train_df,x='Loan_Status',ax=axes[idx])

print(train_df[numerical_columns].describe())
plt.subplots_adjust(hspace=1)

#### Encoding categrical Features: ##########
train_df_encoded = pd.get_dummies(train_df,drop_first=True)
train_df_encoded.head()

########## Split Features and Target Varible ############
X = train_df_encoded.drop(columns='Loan_Status_Y')
y = train_df_encoded['Loan_Status_Y']

################# Splitting into Train -Test Data #######
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify =y,random_state =42)
############### Handling/Imputing Missing values #############
from sklearn.impute import SimpleImputer
imp = SimpleImputer(strategy='mean')
imp_train = imp.fit(X_train)
X_train = imp_train.transform(X_train)
X_test_imp = imp_train.transform(X_test)

"""Model 1: Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score,f1_score


tree_clf = DecisionTreeClassifier()
tree_clf.fit(X_train,y_train)
y_pred = tree_clf.predict(X_train)
print("Training Data Set Accuracy: ", accuracy_score(y_train,y_pred))
print("Training Data F1 Score ", f1_score(y_train,y_pred))

print("Validation Mean F1 Score: ",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())
print("Validation Mean Accuracy: ",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean())

"""Training Accuracy > Test Accuracy with default settings of Decision Tree classifier. Hence, model is overfit. We will try some Hyper-parameter tuning

**First let's try tuning 'Max_Depth' of tree**
"""

training_accuracy = []
val_accuracy = []
training_f1 = []
val_f1 = []
tree_depths = []

for depth in range(1,20):
    tree_clf = DecisionTreeClassifier(max_depth=depth)
    tree_clf.fit(X_train,y_train)
    y_training_pred = tree_clf.predict(X_train)

    training_acc = accuracy_score(y_train,y_training_pred)
    train_f1 = f1_score(y_train,y_training_pred)
    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()
    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()

    training_accuracy.append(training_acc)
    val_accuracy.append(val_mean_accuracy)
    training_f1.append(train_f1)
    val_f1.append(val_mean_f1)
    tree_depths.append(depth)


Tuning_Max_depth = {"Training Accuracy": training_accuracy, "Validation Accuracy": val_accuracy, "Training F1": training_f1, "Validation F1":val_f1, "Max_Depth": tree_depths }
Tuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)

plot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name="Values")
fig,ax = plt.subplots(figsize=(15,5))
sns.pointplot(x="Max_Depth", y="Values",hue="Metrics", data=plot_df,ax=ax)

"""*we can conclude that keeping 'Max_Depth' = 3 will yield optimum Test accuracy and F1 score Optimum Test Accuracy ~ 0.805; Optimum F1 Score: ~0.7*

**Visulazing Decision Tree with Max Depth = 3**
"""

import graphviz
from sklearn import tree

tree_clf = tree.DecisionTreeClassifier(max_depth = 3)
tree_clf.fit(X_train,y_train)
dot_data = tree.export_graphviz(tree_clf,feature_names = X.columns.tolist())
graph = graphviz.Source(dot_data)
graph

"""We could see that some of the leafs have less than 5 samples hence our classifier might overfit. We can sweep hyper-parameter 'min_samples_leaf' to further improve test accuracy by keeping max_depth to 3"""

training_accuracy = []
val_accuracy = []
training_f1 = []
val_f1 = []
min_samples_leaf = []
import numpy as np
for samples_leaf in range(1,80,3): ### Sweeping from 1% samples to 10% samples per leaf
    tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = samples_leaf)
    tree_clf.fit(X_train,y_train)
    y_training_pred = tree_clf.predict(X_train)

    training_acc = accuracy_score(y_train,y_training_pred)
    train_f1 = f1_score(y_train,y_training_pred)
    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()
    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()

    training_accuracy.append(training_acc)
    val_accuracy.append(val_mean_accuracy)
    training_f1.append(train_f1)
    val_f1.append(val_mean_f1)
    min_samples_leaf.append(samples_leaf)


Tuning_min_samples_leaf = {"Training Accuracy": training_accuracy, "Validation Accuracy": val_accuracy, "Training F1": training_f1, "Validation F1":val_f1, "Min_Samples_leaf": min_samples_leaf }
Tuning_min_samples_leaf_df = pd.DataFrame.from_dict(Tuning_min_samples_leaf)

plot_df = Tuning_min_samples_leaf_df.melt('Min_Samples_leaf',var_name='Metrics',value_name="Values")
fig,ax = plt.subplots(figsize=(15,5))
sns.pointplot(x="Min_Samples_leaf", y="Values",hue="Metrics", data=plot_df,ax=ax)

"""We will choose Min_Samples_leaf to 35 to improve test accuracy.

Let's use this Decision Tree classifier on unseen test data and evaluate Test Accuracy, F1 Score and Confusion Matrix
"""

testing_accuracy = []
val_accuracy_test = []
testing_f1 = []
val_f1_test = []
tree_depths_test = []

for depth in range(1,20):
    tree_clf = DecisionTreeClassifier(max_depth=depth)
    tree_clf.fit(X_test_imp,y_test)
    y_testing_pred = tree_clf.predict(X_test_imp)
    #print(X_test_imp)
    if depth == 19:
      print(X_test_imp.shape)
      print("Predicted Output Shape", y_testing_pred.shape)
      print("Predicted Output \n",y_testing_pred)
      X_test_imp = np.arange(0,1722).reshape((123, 14))
      X_test_imp_Single_Entry = X_test_imp[0].reshape(1,-1)
      y_testing_pred_Single_Entry = tree_clf.predict(X_test_imp_Single_Entry)
      print("Single Entry Loan Prediction as \n",y_testing_pred_Single_Entry)
    testing_acc = accuracy_score(y_test,y_testing_pred)
    test_f1 = f1_score(y_test,y_testing_pred)
    val_mean_f1_test = cross_val_score(tree_clf,X_test_imp,y_test,cv=5,scoring='f1_macro').mean()
    val_mean_accuracy_test = cross_val_score(tree_clf,X_test_imp,y_test,cv=5,scoring='accuracy').mean()

    testing_accuracy.append(testing_acc)
    val_accuracy_test.append(val_mean_accuracy_test)
    testing_f1.append(test_f1)
    val_f1_test.append(val_mean_f1_test)
    tree_depths_test.append(depth)

testing_accuracy = []
val_accuracy_test = []
training_f1_test = []
val_f1_test = []
min_samples_leaf = []

for samples_leaf in range(1,80,3): ### Sweeping from 1% samples to 10% samples per leaf
    tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = samples_leaf)
    tree_clf.fit(X_test_imp,y_test)
    y_testing_pred = tree_clf.predict(X_test_imp)

    testing_acc = accuracy_score(y_test,y_testing_pred)
    test_f1 = f1_score(y_test,y_testing_pred)
    val_mean_f1 = cross_val_score(tree_clf,X_test_imp,y_test,cv=5,scoring='f1_macro').mean()
    val_mean_accuracy = cross_val_score(tree_clf,X_test_imp,y_test,cv=5,scoring='accuracy').mean()

    testing_accuracy.append(testing_acc)
    val_accuracy_test.append(val_mean_accuracy)
    training_f1_test.append(test_f1)
    val_f1_test.append(val_mean_f1)
    min_samples_leaf.append(samples_leaf)

from sklearn.metrics import confusion_matrix
tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = 35)
tree_clf.fit(X_train,y_train)
y_pred = tree_clf.predict(X_test_imp)
print("Test Accuracy: ",accuracy_score(y_test,y_pred))
print("Test F1 Score: ",f1_score(y_test,y_pred))
print("Confusion Matrix on Test Data")
pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)

"""Mis-classifications
It can be seen that majority of the misclassifications are happening because of Loan Reject applicants being classified as Accept.

# This is formatted as code


**Please upvote the kernel, if you like this work**

# Summary



![Loan](https://global-uploads.webflow.com/636bdbebfc681f083e923f81/63861e92f38f01f5177e5405_613f03d6c0edc42e66cdcd32_A%2520Guide%2520to%2520Automating%2520Loan%2520Document%2520Data%2520Extraction%2520for%2520Lenders%2520Main%2520Image.jpeg)


#### ***Predicting loan default is a common problem in the financial industry and can be addressed using machine learning.By using this approach, you can quickly build a model that accurately predicts loan default with a low amount of code, making it a convenient and efficient solution for financial institutions***

______________________

# Build a frontend App to deploy and implement
"""

#! pip install streamlit



  ################### Importing Libraries ######################
import pandas as pd

train_df = pd.read_csv(r"C:\Users\sreeb\Downloads\AIMLAcademics\000End_to_End_Real_Time_Projects\DomainBasedDataScienceProjects\BankingDomain\LoanPredictionProblem\DataSets\train.csv")
train_df.info()

  ############ Count number of Categorical and Numerical Columns ######################
train_df = train_df.drop(columns=['Loan_ID']) ## Dropping Loan ID
categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']
  #categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Loan_Amount_Term']

print(categorical_columns)
numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']
print(numerical_columns)


  #### Encoding categrical Features: ##########
train_df_encoded = pd.get_dummies(train_df,drop_first=True)
train_df_encoded.head()

  ########## Split Features and Target Varible ############
X = train_df_encoded.drop(columns='Loan_Status_Y')
y = train_df_encoded['Loan_Status_Y']

  ################# Splitting into Train -Test Data #######
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify =y,random_state =42)
  ############### Handling/Imputing Missing values #############
from sklearn.impute import SimpleImputer
imp = SimpleImputer(strategy='mean')
imp_train = imp.fit(X_train)
X_train = imp_train.transform(X_train)
X_test_imp = imp_train.transform(X_test)

  ###############################################################

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score,f1_score


#tree_clf = DecisionTreeClassifier()
#tree_clf.fit(X_train,y_train)
#y_pred = tree_clf.predict(X_train)
#print("Training Data Set Accuracy: ", accuracy_score(y_train,y_pred))
#print("Training Data F1 Score ", f1_score(y_train,y_pred))

#print("Validation Mean F1 Score: ",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())
#print("Validation Mean Accuracy: ",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean())

import streamlit as st

###############################################################
def handle_Submit(SampleNumber):
    testing_accuracy = []
    val_accuracy_test = []
    training_f1_test = []
    val_f1_test = []
    min_samples_leaf = []
    for samples_leaf in range(1,80,3): ### Sweeping from 1% samples to 10% samples per leaf
        if samples_leaf == 52:
        ################# Splitting into Train -Test Data #######
          from sklearn.model_selection import train_test_split
          X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify =y,random_state =42)
        ############### Handling/Imputing Missing values #############
          from sklearn.impute import SimpleImputer
          imp = SimpleImputer(strategy='mean')
          imp_train = imp.fit(X_train)
          X_train = imp_train.transform(X_train)
          X_test_imp = imp_train.transform(X_test)
          tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = samples_leaf)
          tree_clf.fit(X_test_imp,y_test)
          y_testing_pred = tree_clf.predict(X_test_imp)
          print(X_test_imp.shape)
          print("Predicted Output Shape", y_testing_pred.shape)
          print("Predicted Output \n",y_testing_pred)
          if (0<=SampleNumber<35):
             X_test_imp = np.arange(0,1722).reshape((123, 14))
             X_test_imp_Single_Entry = X_test_imp[SampleNumber].reshape(1,-1)
             y_testing_pred_Single_Entry = tree_clf.predict(X_test_imp_Single_Entry)
             print("Single Entry Loan Prediction for the entry number "+str(SampleNumber)+" is \n",y_testing_pred_Single_Entry)
             print(type(y_testing_pred_Single_Entry))
               #" ".join(str(x) for x in y_testing_pred_Single_Entry)
             #if y_testing_pred_Single_Entry == ['True']:
             return ("Loan approval status for the Customer with the entry number "+str(SampleNumber)+"  is"+ str(y_testing_pred_Single_Entry))
             #elif y_testing_pred_Single_Entry == ['False']:
                   #return ("Loan for the Customer with the entry number "+str(SampleNumber)+"  is Not Approved")

st.title("Loan Approval Prediction Model")
SampleNumber = st.number_input("Enter a SampleNumber between 0 and 34", min_value=0, max_value=34)
st.write(handle_Submit(SampleNumber))